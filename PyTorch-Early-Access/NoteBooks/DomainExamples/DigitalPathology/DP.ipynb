{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "# \u003ccenter\u003eNotebook for Digital Pathology on Clara Train SDK \n",
        "\n",
        "## Description\n",
        "Clara Train SDK comes with many models for different tasks. \n",
        "This notebook will go through the usecase of automated detection of metastases from histopathology whole slide images (WSIs).\n",
        "This notebooks walks you throug :\n",
        "1. Downloading the data\n",
        "2. preprocessing the labels\n",
        "3. Introducing [cuCIM](https://github.com/rapidsai/cucim/), an extensible toolkit designed to provide GPU accelerated I/O to load WSI images.\n",
        "4. Training a model \n",
        "5. Inferring \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Method Settings\n",
        "All the data used to train, validate, and test this model is from \n",
        "[Camelyon-16 Challenge](https://camelyon16.grand-challenge.org/).\n",
        "\n",
        "The detection task is formulated as classification: determining if an arbitary 224x224x3 RGB patch sampled from the WSI is tumor or normal.\n",
        "\n",
        "We adopted [NCRF](https://github.com/baidu-research/NCRF) method\u0027s way of patch sampling, please refer to the corresponding website for more information.\n",
        "\n",
        "Training is performed on patch-label pairs, which are sampled from WSI with tumor delineations.  \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/workflow.png\" width\u003d\"600\"/\u003e\u003c/left\u003e\n",
        "\n",
        "The prediction map is generated in a sliding-window manner.  \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/prediction.png\" width\u003d\"300\"/\u003e \u003cimg src\u003d\"screenShots/image.png\" width\u003d\"300\" align\u003d\"left\"/\u003e\u003c/left\u003e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Disclamer \n",
        "This notebook will use four WSI files to illustrate the training process.\n",
        "This is **ONLY** intended to show the user how to get started. \n",
        "For the model please train on full data (download process can take several days), or download the trained model from NGC release. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Prerequisites\n",
        "- Familiarity with Clara Train main concepts. See [Getting Started Notebook](../../GettingStarted/GettingStarted.ipynb)\n",
        "- Familiarity with Bring your own component. See [GBring your own component notebook](../../GettingStarted/BYOC.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## \u003ccenter\u003eNow Let\u0027s Get Started with Data Preparation and Clara Pathology Detection MMAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 1. Download Data\n",
        "First let\u0027s setup directories for the data, this will create /Data folder in the current path, with all necessary subfolders "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root\u003dos.getcwd()\n",
        "# DataDirRoot\u003droot+\"/Data/\"\n",
        "DataDirRoot\u003d\"/claraDevDay/Data/DP_CAMELYON16/\"\n",
        "print(\"Data dir root: \", DataDirRoot)\n",
        "\n",
        "DataDirJson\u003dDataDirRoot+\"jsons/\"\n",
        "DataDirCoordRaw\u003dDataDirRoot+\"coordsRaw/\"\n",
        "DataDirCoord\u003dDataDirRoot+\"coords/\"\n",
        "DataDirWSI\u003dDataDirRoot+\"WSI/\"\n",
        "DataDirLoc\u003dDataDirRoot+\"LocLabel/\"\n",
        "\n",
        "os.makedirs(DataDirJson, exist_ok\u003dTrue)\n",
        "os.makedirs(DataDirCoordRaw, exist_ok\u003dTrue)\n",
        "os.makedirs(DataDirCoord, exist_ok\u003dTrue)\n",
        "os.makedirs(DataDirWSI, exist_ok\u003dTrue)\n",
        "os.makedirs(DataDirLoc, exist_ok\u003dTrue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "To be specific, below folder will be downloaded\n",
        "- /WSI stores the histopathology images\n",
        "- /jsons stores the ground truth annotation in json format\n",
        "- /coordsRaw stores all patch sample locations\n",
        "\u003cbr\u003e\n",
        "below folders would be generated:\n",
        "- /coords will be generated from /coordsRaw by keeping only the locations for the downloaded WSIs (4 in this notebook).\n",
        "- /LocLabel contains the full sample info, and will be generated based on /coords and /jsons  \n",
        "\u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/folders.png\" width\u003d\"200\"/\u003e\u003c/left\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 1.1 Download WSIs  \n",
        "You can download all the images for \"CAMELYON16 data set\" from various sources listed \n",
        "[here](https://camelyon17.grand-challenge.org/Data/). Downloading all data can take several days.\n",
        "\n",
        "Due to time constraint and for simplicity, this notebook only download 4 WSIs: 2 tumors and 2 normals for training and validation respectively. Let\u0027s download them from FTP below. \u003cbr\u003e\n",
        "**Please note: This download could take some time, in total 3.3 GB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! pip install progressbar2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from progressbar import ProgressBar, Percentage, Bar, ETA, FileTransferSpeed\n",
        "def download_file_with_progressbar(data):\n",
        "    f.write(data) \n",
        "    global bar\n",
        "    bar +\u003d len(data)\n",
        "\n",
        "import ftplib\n",
        "import os\n",
        "def download_camelyon16_image(filename):\n",
        "    filename \u003d filename.lower()\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"The image [{filename}] already exist locally.\")\n",
        "    else:\n",
        "        print(f\"Downloading \u0027{filename}\u0027...\")\n",
        "        prefix \u003d filename.split(\"_\")[0].lower()\n",
        "        if prefix \u003d\u003d \"test\":\n",
        "            folder_name \u003d \"testing/images\"\n",
        "        elif prefix in [\"normal\", \"tumor\"]:\n",
        "            folder_name \u003d f\"training/{prefix}\"\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"\u0027{filename}\u0027 not found on the server.\"\n",
        "                \" File name should be like \u0027test_001.tif\u0027, \u0027tumor_001.tif\u0027, or \u0027normal_001.tif\u0027\"\n",
        "            )\n",
        "        path \u003d f\"gigadb/pub/10.5524/100001_101000/100439/CAMELYON16/{folder_name}/\"\n",
        "        ftp \u003d ftplib.FTP(\"parrot.genomics.cn\")\n",
        "        ftp.login(\"anonymous\", \"\")\n",
        "        filepath\u003dpath+filename\n",
        "        print(\"Downloading \",filepath)\n",
        "        size \u003d ftp.size(filepath)\n",
        "        global bar\n",
        "        bar \u003d ProgressBar(widgets\u003d[\u0027Downloading: \u0027, Percentage(), \u0027 \u0027,\n",
        "                        Bar(marker\u003d\u0027#\u0027,left\u003d\u0027[\u0027,right\u003d\u0027]\u0027),\n",
        "                        \u0027 \u0027, ETA(), \u0027 \u0027, FileTransferSpeed()], maxval\u003dsize)\n",
        "        bar.start()    \n",
        "        global f\n",
        "        f \u003d open(filename, \u0027wb\u0027)  \n",
        "        #ftp.cwd(path)\n",
        "        #ftp.retrbinary(\"RETR \" + filename, open(filename, \"wb\").write)\n",
        "        ftp.retrbinary(\"RETR \" + filepath, download_file_with_progressbar)\n",
        "\n",
        "        ftp.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "%cd $DataDirWSI\n",
        "download_camelyon16_image(\"tumor_091.tif\")\n",
        "download_camelyon16_image(\"tumor_107.tif\")\n",
        "download_camelyon16_image(\"normal_042.tif\")\n",
        "download_camelyon16_image(\"normal_150.tif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "\u003cbr\u003e\n",
        "Check that the files were downloaded in the correct folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "!ls $DataDirWSI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 1.2. Download annotation json file\n",
        "Annotation information are adopted from \n",
        "[NCRF/jsons](https://github.com/baidu-research/NCRF/tree/master/jsons).\n",
        "Cell below will download the needed files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "jsonURL\u003d\"https://raw.githubusercontent.com/baidu-research/NCRF/master/jsons/\"\n",
        "\n",
        "wget_URL\u003djsonURL+\"train/Tumor_091.json\"\n",
        "!wget $wget_URL -P $DataDirJson --no-check-certificate\n",
        "wget_URL\u003djsonURL+\"valid/Tumor_107.json\"\n",
        "!wget $wget_URL -P $DataDirJson --no-check-certificate\n",
        "wget_URL\u003djsonURL+\"train/Normal_042.json\"\n",
        "!wget $wget_URL -P $DataDirJson --no-check-certificate\n",
        "wget_URL\u003djsonURL+\"valid/Normal_150.json\"\n",
        "!wget $wget_URL -P $DataDirJson --no-check-certificate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 1.3. Download patch coords\n",
        "Location information for training/validation patches are adopted from \n",
        "[NCRF/coords](https://github.com/baidu-research/NCRF/tree/master/coords).\n",
        "Cell below will download the needed files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "coordsURL\u003d\"https://raw.githubusercontent.com/baidu-research/NCRF/master/coords/\"\n",
        "\n",
        "wget_URL\u003dcoordsURL+\"tumor_train.txt\"\n",
        "!wget $wget_URL -P $DataDirCoordRaw --no-check-certificate\n",
        "wget_URL\u003dcoordsURL+\"tumor_valid.txt\"\n",
        "!wget $wget_URL -P $DataDirCoordRaw --no-check-certificate\n",
        "wget_URL\u003dcoordsURL+\"normal_train.txt\"\n",
        "!wget $wget_URL -P $DataDirCoordRaw --no-check-certificate\n",
        "wget_URL\u003dcoordsURL+\"normal_valid.txt\"\n",
        "!wget $wget_URL -P $DataDirCoordRaw --no-check-certificate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "Let\u0027s only keep the location info for the WSIs we downloaded "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "cmd\u003d\"grep Tumor_091 \"+DataDirCoordRaw+\"tumor_train.txt\"+\" \u003e \"+DataDirCoord+\"tumor_train.txt\"\n",
        "! $cmd\n",
        "cmd\u003d\"grep Tumor_107 \"+DataDirCoordRaw+\"tumor_valid.txt\"+\" \u003e \"+DataDirCoord+\"tumor_valid.txt\"\n",
        "! $cmd\n",
        "cmd\u003d\"grep Normal_042 \"+DataDirCoordRaw+\"normal_train.txt\"+\" \u003e \"+DataDirCoord+\"normal_train.txt\"\n",
        "! $cmd\n",
        "cmd\u003d\"grep Normal_150 \"+DataDirCoordRaw+\"normal_valid.txt\"+\" \u003e \"+DataDirCoord+\"normal_valid.txt\"\n",
        "! $cmd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## 2. Data Preparation for MMAR Training\n",
        "\n",
        "The current sample location information, e.g. /coords/tumor_train.txt has information below:  \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/coords.png\" height\u003d\"200\"/\u003e\u003c/left\u003e\n",
        "\n",
        "In fact, at each location, a 728x728 patch will be sampled, which will further be decomposed to a 3x3 grid of 224x224 patches. \n",
        "Therefore, we need to convert the downloaded patch coords to json that works with Clara MMAR with the following two steps:\n",
        "1. Read NCRF coords and annotation jsons, output full index/label information: `prepare_train_data.sh` is used to generate the LocLabel files needed for training and validation from /coords and /jsons listed above. It will append the labels after each filename + coordinate pairs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "MMAR_ROOT\u003droot+\"/MMAR_DP/\"\n",
        "print (\"setting MMAR_ROOT \u003d\",MMAR_ROOT)\n",
        "%ls $MMAR_ROOT\n",
        "!chmod 777 $MMAR_ROOT/commands/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/prepare_train_data.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "It will identify all 9 labels at each sample location \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/loclabel.png\" height\u003d\"200\"/\u003e\u003c/left\u003e\n",
        "\n",
        "Cell below will display head of each file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "!head -5 $DataDirLoc/tumor_train.txt\n",
        "!head -5 $DataDirLoc/normal_train.txt\n",
        "!head -5 $DataDirLoc/tumor_valid.txt\n",
        "!head -5 $DataDirLoc/normal_valid.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "2. Then we combine the txt files to a single json which will be used by Clara MMAR using `prepare_json.sh`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/prepare_json.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "This will produce a single json file /Data/datalist.json that will be used by Clara MMAR "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! head -30 $DataDirRoot/datalist.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "\u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/clara_json.png\" width\u003d\"400\"/\u003e\u003c/left\u003e\n",
        "\n",
        "As shown above, each sample Clara accepts for training/validation has the information on WSI path, sample location, and the 9 corresponding labels. Now we have the necessary input for the training/validation pipeline: \n",
        "1. path to the folder containing all WSIs\n",
        "2. json file listing the location and label information for training patches.\n",
        "\n",
        "The paths are set in /config/environment.json \u003cbr\u003e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 3. Clara MMAR Training \n",
        "### Model Overview\n",
        "The model is based on ResNet18 with the last fully connected layer replaced by a 1x1 convolution layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": "### WSI Reader \u003cspan style\u003d\"color:red\"\u003e(New in V4)\u003c/span\u003e\nWe recommend using [cuCIM](https://github.com/rapidsai/cucim/), \nan extensible toolkit designed to provide GPU accelerated I/O, \ncomputer vision \u0026 image processing primitives for N-Dimensional images with a focus on biomedical imaging, \nto load WSI images. [OpenSlide](https://openslide.org/), the popular WSI-reader, is also provided for convenience. \nUsers can choose between the two using the option in config files\u003cbr\u003e\n\n\u003cleft\u003e\u003cimg src\u003d\"screenShots/wsireader.png\" width\u003d\"700\"/\u003e\u003c/left\u003e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 3.1 Training\n",
        "Training can be performed regularly, or with [Smart Cache](https://docs.nvidia.com/clara/tlt-mi/nvmidl/additional_features/smart_cache.html) mechanism.\n",
        "\n",
        "Before we get started lets check that we have an NVIDIA GPU available in the docker by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "#### 3.1.1 Regular Training\n",
        "Then we can start regular training (w/o smart cache mechanism). For this example, we will train for 4 epochs with /commands/train.sh. \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/train.png\" width\u003d\"300\"/\u003e\u003c/left\u003e    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/train.sh "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "#### 3.1.2 Smart Cache Training\n",
        "We can also use smart cache training, which can be especially helpful for pathology applications due to the massive amount of patches used during training. For this example, we will cache 2000 samples and train for 40 smart cache epoches, with /commands/train_smartcache.sh, which will save the output models to models_sc/ \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/train_sc.png\" width\u003d\"300\"/\u003e\u003c/left\u003e    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/train_smartcache.sh "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 3.2 Scores and Results\n",
        "Example shown here uses the minimum amount of WSIs for simplicity and illustrating how the pipeline works. \n",
        "Therefore, the resulting model is a dummy one that is not useful. \n",
        "You can either download and train on all the data, or use our model form NGC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 3.3 Model Export\n",
        "Model will be exported to torch script format "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/export.sh "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 3.4 Validation\n",
        "To run validation on patches, simply run cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/validate.sh "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "### 3.5 Inference on a WSI\n",
        "Output of the network itself is the probability map of the input patch.\n",
        "\n",
        "Inference can then be performed on WSI in a sliding window manner with specified stride. \n",
        "\n",
        "A foreground mask is needed to specify the region where the inference will be performed on, given that background region which contains no tissue at all can occupy a significant portion of a WSI. \n",
        "\n",
        "Otsu thresholding in `prepare_inference_data.sh` is used to generate foreground masks that will be used to reduce computation burden during inference. The input is the test image, and output is its foreground mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/prepare_inference_data.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "With the foreground map, inference can be performed on the WSI. Output of the inference pipeline is a probability map of size 1/stride of original WSI size.\n",
        "In datalist.json, the inference on full WSIs is specified under \"testing\" \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/testing.png\" width\u003d\"300\"/\u003e\u003c/left\u003e \n",
        "\n",
        "Inference will be performed by /commands/infer.sh, results will be saved to /eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! $MMAR_ROOT/commands/infer.sh "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 4. Clara MMAR Performance on Full Camelyon16 Data \n",
        "For reference, we list the performance of Clara Paholoygy Detection MMAR on full Camelyon16 data. The experiments are performed using a single 32 GB V100 GPU on a [NVIDIA DGX-2 System](https://www.nvidia.com/en-us/data-center/dgx-2/) \n",
        "\n",
        "Benchmarking with [NCRF](https://github.com/baidu-research/NCRF) (Resnet18 baseline, without CRF), which was implemented with pytorch and OpenSlide, we used the same patch locations and performed same 20 epoch experiments.  \n",
        "\n",
        "Summary for all experiments (time in hours), note that NCRF need to generate PNG files for patches, while Clara loads the patches on the fly:\n",
        " \n",
        " \n",
        " Experiment | uses pre-trained \u003cbr/\u003emodel | AMP | Library | 2D Patch \u003cbr/\u003eGeneration | Training \u003cbr/\u003e(20 epochs) | Total \u003cbr/\u003eTraining Time | Speedup | Best \u003cbr/\u003eModel | FROC on 48 \u003cbr/\u003eTest Cases | Training Speedup \u003cbr/\u003eat Best Model\n",
        " :--- | :---: | :---: | :---: | :---: | :---:| :----:| :----:| :----:| :----:| :---:\n",
        " NCRF | No | NoAmp | - | 10 | 70.5 | 80.5 | 1x | 35.5 | 0.69 | 1x\n",
        " Clara | yes | Amp | OpenSlide | N/A | 48.5 | 48.5 | 1.66x | 5.5 | 0.71 | 8x\n",
        " Clara | yes | Amp | **cuCIM** | N/A | **39.5** | **39.5** | **2x** | **4.5** | **0.72** | **10x**\n",
        "\n",
        "\u003cbr\u003e\n",
        "\n",
        "**For inference**\n",
        " \n",
        "Experiment | pre-trained | Inference | speedup  \n",
        " --- | --- |  ---:| ---:\n",
        " NCRF | - | 26.5 | 1x\n",
        " Clara | OpenSlide | 2 | 13x\n",
        " Clara | cuCIM | 2 | 13x\n",
        "\n",
        "\n",
        "Inference speedup is due to 2 factors:\n",
        "1. Optimized inference pipeline by Clara \n",
        "2. Larger batch size used (80 v.s. 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "Training curve of NCRF:\u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/ncrf.png\" width\u003d\"600\"/\u003e\u003c/left\u003e \n",
        "\n",
        "Training curve of Clara regular training:\u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/reg_train_loss.png\" width\u003d\"600\"/\u003e\u003c/left\u003e \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/reg_val_acc.png\" width\u003d\"600\"/\u003e\u003c/left\u003e \n",
        "\n",
        "Training curve of Clara training with smart cache:\u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/sc_train_loss.png\" width\u003d\"600\"/\u003e\u003c/left\u003e \u003cbr\u003e\n",
        "\u003cleft\u003e\u003cimg src\u003d\"screenShots/sc_val_acc.png\" width\u003d\"600\"/\u003e\u003c/left\u003e "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "# Exercise \n",
        "### 1. Train using open slide \n",
        "You can compare performance against openslide.\n",
        "For this you would to install openslide packages by running cell below \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Install openslide packages  \n",
        "!apt-get -y install openslide-tools\n",
        "!apt-get -y install python-openslide\n",
        "!python3 -m pip install --upgrade pip\n",
        "!pip install openslide-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        " \n",
        "You can change the data loader to use open slide, \n",
        "for this you would need to change the config_train.json or the \n",
        "\n",
        "you simply need to change \n",
        "```\n",
        "        \"image_reader_name\": \"cuclaraimage\"\n",
        "``` \n",
        "to \n",
        "```\n",
        "        \"image_reader_name\": \"openslide\"\n",
        "``` \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "stem_cell": {
      "cell_type": "raw",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": "\u003c!--- SPDX-License-Identifier: Apache-2.0 --\u003e\n"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}