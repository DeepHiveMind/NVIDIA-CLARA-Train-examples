{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Running FL with secure aggregation using homomorphic encryption\n",
    "\n",
    "This notebook will walk you through how to setup FL with homomorphic encryption (HE). \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this notebook, please make yourself familiar with other FL notebooks in this repo.\n",
    "\n",
    "- (Optional) Look at the introduction Notebook for [Federated Learning with Clara Train SDK](FederatedLearning.ipynb).\n",
    "- (Optional) Look at [Client Notebook](Client.ipynb).\n",
    "- (Optional) Look at [Admin Notebook](Admin.ipynb).\n",
    "- Run [Provisioning Notebook](Provisioning.ipynb) and started the server.\n",
    "\n",
    "Make sure the project.yml used for provision contains these HE related settings:\n",
    "\n",
    "    # homomorphic encryption\n",
    "    he:\n",
    "      lib: tenseal\n",
    "      config:\n",
    "        poly_modulus_degree: 8192\n",
    "        coeff_mod_bit_sizes: [60, 40, 40]\n",
    "        scale_bits: 40\n",
    "        scheme: CKKS\n",
    "        \n",
    "*Note:* These settings are recommended and should work for most tasks but could be further optimized depending on your specific model architecture and machine learning task. See this [tutorial on the CKKS scheme](https://github.com/OpenMined/TenSEAL/blob/master/tutorials/Tutorial%202%20-%20Working%20with%20Approximate%20Numbers.ipynb) and [benchmarking](https://github.com/OpenMined/TenSEAL/blob/master/tutorials/Tutorial%203%20-%20Benchmarks.ipynb) for more information of different settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "##### Option 1 \n",
    "This notebook uses a sample dataset (ie. a single image volume of the spleen dataset) provided in the package to train a small neural network for a few epochs. \n",
    "This single file is duplicated 32 times for the training set and 9 times for the validation set to mimic the full spleen dataset. \n",
    "\n",
    "##### Option 2  \n",
    "You could do minor changes as recommended in the excersise section to train on the spleen segmentation task. The dataset used is Task09_Spleen.tar from \n",
    "the [Medical Segmentation Decathlon](http://medicaldecathlon.com/). \n",
    "Prior to running this notebook the data should be downloaded following \n",
    "the steps in [Data Download Notebook](../../Data_Download.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer  \n",
    "We will be training a small networks so that both clients can fit the model on 1 gpu. \n",
    "Training will run for a couple of epochs, in order to show the concepts, we are not targeting accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets get started\n",
    "In order to learn how FL works with homomorphic encryption (HE) in Clara Train SDK we will first give some background on what homomorphic encryption is and how the MMAR configurations need to be modifyed to enable it.\n",
    "<br><img src=\"./screenShots/homomorphic_encryption.png\" alt=\"Drawing\" style=\"height: 450px;\"/><br> \n",
    "\n",
    "## TODOs: \n",
    "\n",
    "### Explain new HE components*\n",
    "- Cite TenSEAL and Microsoft SEAL - done\n",
    "- Link to API doc?\n",
    "- Encryptor (all layers, partial, regex) - done\n",
    "- Decryptor - done\n",
    "- Just in time HE aggregator - done\n",
    "- HE ShareableGenerator - done\n",
    "- HE Persistor - done\n",
    "- Cross-site validation with HE - done\n",
    "\n",
    "### Show functionality\n",
    "- Show that server connot decrypt - done\n",
    "- Show how client can decrypt - done\n",
    "- Show how client can decrypt global model and save as torch checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New HE components\n",
    "\n",
    "We implemented secure aggregation during FL with homomorphic encryption using the [TenSEAL library](https://github.com/OpenMined/TenSEAL) by OpenMined, a convienent wrapper around [Microsoft SEAL](https://github.com/microsoft/SEAL). Both libraries are available as open-source and provide an implementation of [\"Homomorphic encryption for arithmetic of approximate numbers\"](https://eprint.iacr.org/2016/421.pdf), aka the \"CKKS\" scheme, which was proposed as a solution for [encrypted machine learning](https://en.wikipedia.org/wiki/Homomorphic_encryption#Fourth-generation_FHE) and which we use for these FL experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration files in `adminMMAR_HE` use the following new componets that are needed on top or instead of standard FL components used in Clara Train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client-side \n",
    "See `config_fed_client.json`:\n",
    "\n",
    "### `HEModelEncryptor`\n",
    "A filter to the encrypt Shareable object that being sent to the server.\n",
    "\n",
    "```\n",
    "Args:\n",
    "    tenseal_context_file: tenseal context files containing encryption keys and parameters\n",
    "    encrypt_layers: if not specified (None), all layers are being encrypted;\n",
    "                    if list of variable/layer names, only specified variables are encrypted;\n",
    "                    if string containing regular expression (e.g. \"conv\"), only matched variables are being encrypted.\n",
    "    aggregation_weights: dictionary of client aggregation `{\"client1\": 1.0, \"client2\": 2.0, \"client3\": 3.0}`;\n",
    "                         defaults to a weight of 1.0 if not specified.\n",
    "    weigh_by_local_iter: If true, multiply client weights on first before encryption (default: `True` which is recommended for HE)\n",
    "```\n",
    "\n",
    "HE will increase the message sizes when encrypting the model updates of each client. One can choose to not encrypt all layers but specify which layers to enrypt, see arg `encrypt_layers`.\n",
    "\n",
    "To choose the layer names for a given model, one can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.unit0.conv.weight\n",
      "model.0.conv.unit0.conv.bias\n",
      "model.0.conv.unit0.adn.A.weight\n",
      "model.0.conv.unit1.conv.weight\n",
      "model.0.conv.unit1.conv.bias\n",
      "model.0.conv.unit1.adn.A.weight\n",
      "model.1.submodule.0.conv.unit0.conv.weight\n",
      "model.1.submodule.0.conv.unit0.conv.bias\n",
      "model.1.submodule.0.conv.unit0.adn.A.weight\n",
      "model.1.submodule.0.conv.unit1.conv.weight\n",
      "model.1.submodule.0.conv.unit1.conv.bias\n",
      "model.1.submodule.0.conv.unit1.adn.A.weight\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "model.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "model.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "model.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "model.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.unit0.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.unit0.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.unit1.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.0.conv.unit1.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit0.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.1.submodule.conv.unit1.adn.A.weight\n",
      "model.1.submodule.1.submodule.1.submodule.2.0.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.2.0.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "model.1.submodule.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "model.1.submodule.1.submodule.2.0.conv.weight\n",
      "model.1.submodule.1.submodule.2.0.conv.bias\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.weight\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.conv.bias\n",
      "model.1.submodule.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "model.1.submodule.2.0.conv.weight\n",
      "model.1.submodule.2.0.conv.bias\n",
      "model.1.submodule.2.1.conv.unit0.conv.weight\n",
      "model.1.submodule.2.1.conv.unit0.conv.bias\n",
      "model.1.submodule.2.1.conv.unit0.adn.A.weight\n",
      "model.2.0.conv.weight\n",
      "model.2.0.conv.bias\n",
      "model.2.1.conv.unit0.conv.weight\n",
      "model.2.1.conv.unit0.conv.bias\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets.unet import UNet\n",
    "\n",
    "# use the same configuration as in adminMMAR_HE\n",
    "net = UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=[16, 32, 64, 128, 256],\n",
    "    strides=[2, 2, 2, 2],\n",
    "    num_res_units=2    \n",
    ")\n",
    "\n",
    "# here, we only print convolutional layers that we might want to encrypt\n",
    "for key in net.state_dict().keys():\n",
    "    if 'conv' in key:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output, our example `config_fed_client.json` chooses three layers to encrypt:\n",
    "\n",
    "```\n",
    "\"outbound_filters\": [\n",
    "  {\n",
    "    \"path\": \"flare.experimental.homomorphic_encryption.he_model_encryptor.HEModelEncryptor\",\n",
    "    \"args\": {\n",
    "      \"encrypt_layers\": [\n",
    "        \"model.0.conv.unit0.conv.weight\",\n",
    "        \"model.1.submodule.1.submodule.1.submodule.2.0.conv.weight\",\n",
    "        \"model.2.1.conv.unit0.conv.weight\"\n",
    "      ],\n",
    "      \"aggregation_weights\": {\n",
    "        \"client1\":  0.4,\n",
    "        \"client2\":  0.6\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HEModelDecryptor`\n",
    "A filter to decrypt Shareable object, i.e. the updated global model received from the server.\n",
    "\n",
    "```\n",
    "Args:\n",
    "    tenseal_context_file: tenseal context files containing decryption keys and parameters\n",
    "```\n",
    "\n",
    "*Note:* The tenseal_context_file for the client will be generated by the provision tool and is part of the startup kit, see [Provisioning](./Provisioning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HEPTModelReaderWriter`\n",
    "\n",
    "This component is used as argument to `ClientTrainer` to reshape the decrypted parameter vectors to the local Pytorch model for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HEEvalDecryptor`\n",
    "Filter to decrypt encrypted Shareable object (i.e. global model(s)) distributed during cross-site validation. Currently, only the global server models are encrypted. Locally best models are shared unencrypted.\n",
    "\n",
    "*Note:* cross-site validation is optional and a client does not need to participate if not wanted.\n",
    "\n",
    "```\n",
    "Args:\n",
    "    tenseal_context_file: tenseal context files containing decryption keys and parameters\n",
    "    defaults to `False` for use during FL training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server-side \n",
    "See `config_fed_server.json`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HEInTimeAccumulateWeightedAggregator`\n",
    "\n",
    "This aggregator can perform federated averaging (i.e. the [`FedAvg`](http://proceedings.mlr.press/v54/mcmahan17a.html) algorithm) in encrypted space. The server doesn't have a key for decryption and only processes the encrypted values sent by the clients.)\n",
    "\n",
    "```\n",
    "Args:\n",
    "    exclude_vars: variable names that should be excluded from aggregation (use regular expression)\n",
    "    aggregation_weights: dictionary of client aggregation `{\"client1\": 1.0, \"client2\": 2.0, \"client3\": 3.0}`;\n",
    "                         defaults to a weight of 1.0 if not specified. Will be ignored if weigh_by_local_iter: False (default for HE)\n",
    "    weigh_by_local_iter: If true, multiply client weights on first in encryption space\n",
    "                         (default: `False` which is recommended for HE, first multiply happens in `HEModelEncryptor`)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HEPTFileModelPersistor`\n",
    "\n",
    "This model persistor is used to save the encrypted models on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HEModelShareableGenerator`\n",
    "\n",
    "ShareableGenerator converts between Shareable and Learnable objects generated with HE. It is used to update the global model weights using the averaged encrypted updates from the clients. The updated global stays encrypted.\n",
    "    \n",
    "```\n",
    "Args:\n",
    "    tenseal_context_file: tenseal context files containing decryption keys and parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running FL experiment with HE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Start server, and clients (if they are not already running)\n",
    "Open four terminals in JupyterLab.\n",
    "\n",
    "In the server terminal run:\n",
    "```\n",
    "cd /claraDevDay/FL/project1/server/startup\n",
    "./start.sh\n",
    "```  \n",
    "In the client1 terminal run:\n",
    "```\n",
    "cd /claraDevDay/FL/project1/client1/startup\n",
    "./start.sh\n",
    "```  \n",
    "In the client2 terminal run:\n",
    "```\n",
    "cd /claraDevDay/FL/project1/client2/startup\n",
    "./start.sh\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Starting Admin Shell\n",
    "In the admin terminal, if you haven't already started the admin console you should to admin folder in side your project and run\n",
    "```\n",
    "cd /claraDevDay/FL/project1/admin/startup\n",
    "./fl_admin.sh\n",
    "``` \n",
    "you should see\n",
    "```\n",
    "Admin Server: localhost on port 5000\n",
    "User Name: `\n",
    "```\n",
    "type `admin@admin.com` \n",
    "\n",
    "Admin Server: localhost on port 8003\n",
    "User Name: admin@admin.com\n",
    "\n",
    "Type ? to list commands; type \"? cmdName\" to show usage of a command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Check server/client status\n",
    "type \n",
    "```\n",
    "> check_status server\n",
    "```\n",
    "to see \n",
    "```\n",
    "FL run number has not been set.\n",
    "FL server status: training not started\n",
    "Registered clients: 2 \n",
    "-------------------------------------------------------------------------------------------------\n",
    "| CLIENT NAME | TOKEN                                | LAST ACCEPTED ROUND | CONTRIBUTION COUNT |\n",
    "-------------------------------------------------------------------------------------------------\n",
    "| client1     | f735c245-ce35-4a08-89e0-0292bb053a9c |                     | 0                  |\n",
    "| client2     | e36db52e-2624-4989-855a-28fa195f58e9 |                     | 0                  |\n",
    "-------------------------------------------------------------------------------------------------\n",
    "```\n",
    "To check on clients type \n",
    "```\n",
    "> check_status client\n",
    "```\n",
    "to see \n",
    "```\n",
    "instance:client1 : client name: client1 token: 3c3d2276-c3bf-40c1-bc02-9be84d7c339f     status: training not started\n",
    "instance:client2 : client name: client2 token: 92806548-5515-4977-894e-612900ff8b1b     status: training not started\n",
    "```\n",
    "To check on folder structure \n",
    "\n",
    "```\n",
    "> info\n",
    "```\n",
    "To see\n",
    "```\n",
    "Local Upload Source: /claraDevDay/FL/project1/admin/startup/../transfer\n",
    "Local Download Destination: /claraDevDay/FL/project1/admin/startup/../transfer\n",
    "Server Upload Destination: /claraDevDay/FL/project1/server/startup/../transfer\n",
    "Server Download Source: /claraDevDay/FL/project1/server/startup/../transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Upload and deploy the MMAR configurations for HE and set FL run number\n",
    "First set a run number (Choose a different one if you don't want to overwrite previous results)\n",
    "```\n",
    "> set_run_number 1\n",
    "```\n",
    "\n",
    "Then, upload the HE MMAR and deploy to server and clients\n",
    "```\n",
    "> upload_folder ../../../adminMMAR_HE\n",
    "> deploy adminMMAR_HE server\n",
    "> deploy adminMMAR_HE client\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Start Training\n",
    "Now you can start training by:\n",
    "\n",
    "1. `> start server`\n",
    "2. `> start client`\n",
    "\n",
    "You can check on the status of the training using:\n",
    "\n",
    "3. `> check_status client` or `> check_status server`  to see \n",
    "\n",
    "```\n",
    "FL run number:1\n",
    "FL server status: training started\n",
    "run number:1    start round:0   max round:2     current round:0\n",
    "min_num_clients:2       max_num_clients:100\n",
    "Registered clients: 2 \n",
    "Total number of clients submitted models for current round: 0\n",
    "-------------------------------------------------------------------------------------------------\n",
    "| CLIENT NAME | TOKEN                                | LAST ACCEPTED ROUND | CONTRIBUTION COUNT |\n",
    "-------------------------------------------------------------------------------------------------\n",
    "| client1     | f735c245-ce35-4a08-89e0-0292bb053a9c |                     | 0                  |\n",
    "| client2     | e36db52e-2624-4989-855a-28fa195f58e9 |                     | 0                  |\n",
    "-------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "4. get logs from server or clients using `cat server log.txt` or `cat client1 log.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Stop Training (if needed ) \n",
    "You could send signals to stop the training if you need to using:\n",
    "- `abort client`\n",
    "- `abort server`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Cross-site validate\n",
    "Once training is completed, you would like to get the validation matrices. \n",
    "This is another area where Clara FL shines. \n",
    "One of the promises of FL is that it enables training more generalizable models due to the more diverse datasets accessed by each client. The off-diagonal values show how well locally best and global models trained in FL generalize across the different client sites. \n",
    "Without Clara, you would need to move either the data or the selected model to each site and run validation at each site separately. \n",
    "With the cross-site validation feature, it is done automatically for you.\n",
    "All you need to do is have the file `config_cross_site_validataion.json` as part of your MMAR, and have set the flag \n",
    "`\"cross_site_validate\": true` in the client section of the config_fed_client.json. \n",
    "These setting is already set up in this example, so all that's left is to\n",
    "\n",
    "Run `validate all` to show the cross-site validation results. You could also run `validate source_site target_site` to see the performance of a certain model on a certain site.\n",
    "\n",
    "You should see something like \n",
    "```\n",
    "validate all\n",
    "{'client1': {'client2': {'validation': {'mean_dice': 0.0637669786810875}}, 'client1': {'validation': {'mean_dice': 0.07123523205518723}}, 'server': {'validation': {'mean_dice': 0.07032141834497452}}}, 'client2': {'client2': {'validation': {'mean_dice': 0.06376668065786362}}, 'client1': {'validation': {'mean_dice': 0.07123514264822006}}, 'server': {'validation': {'mean_dice': 0.07032135874032974}}}}\n",
    "Done [11570 usecs] 2020-09-03 18:49:41.485214\n",
    "``` \n",
    "parsing this json and putting it in a table would look like  \n",
    "\n",
    " _ | Client 1 | Client 2 | Server  \n",
    " :--- | :--- | :---: | --- \n",
    "Client 1 | 0.07123523205518723 | 0.0637669786810875 | 0.07032141834497452\n",
    "Client 2 | 0.07123514264822006 | 0.06376668065786362| 0.07032135874032974"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Server security\n",
    "\n",
    "To illustrate that the server cannot decrypt the messages sent by the client, we can execute this small test script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TenSEAL context from /claraDevDay/FL/project1/server/startup/server_context.tenseal\n",
      "Loaded TenSEAL context from /claraDevDay/FL/project1/client1/startup/client_context.tenseal\n",
      "model: ['model', 'train_conf', 'he_encrypted_layers']\n",
      "3 of 63 layers are encrypted.\n",
      "model.0.conv.unit0.conv.weight is encrypted. Trying to decrypt...\n",
      "<class 'bytes'>\n",
      "Server decryption failed with: the current context of the tensor doesn't hold a secret_key, please provide one as argument!\n",
      "Client decrypted parameters for model.0.conv.unit0.conv.weight:\n",
      "[ 0.06758314 -0.02286799  0.15225781 ...  0.17290769 -0.14571007\n",
      "  0.04857256]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "from flare.experimental.homomorphic_encryption.homomorphic_encrypt import count_encrypted_layers, load_tenseal_context\n",
    "\n",
    "global_model_file = \"/claraDevDay/FL/project1/server/run_1/mmar_server/models/best_FL_global_model.pt\"\n",
    "server_context_file = \"/claraDevDay/FL/project1/server/startup/server_context.tenseal\"\n",
    "client_context_file = \"/claraDevDay/FL/project1/client1/startup/client_context.tenseal\"\n",
    "\n",
    "# load the server and client TenSEAL context files\n",
    "server_ts_ctx = load_tenseal_context(server_context_file)\n",
    "client_ts_ctx = load_tenseal_context(client_context_file)\n",
    "\n",
    "# load the global model saved on the server\n",
    "with open(global_model_file, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "print(\"model:\", list(model.keys()))\n",
    "\n",
    "encrypted_layers = model[\"he_encrypted_layers\"]\n",
    "model = model[\"model\"]\n",
    "\n",
    "count_encrypted_layers(encrypted_layers)\n",
    "\n",
    "# try decrypting the first encrypted layer\n",
    "for encrypted_layer in encrypted_layers:\n",
    "    if encrypted_layer:\n",
    "        print(f\"{encrypted_layer} is encrypted. Trying to decrypt...\")\n",
    "        print(type(model[encrypted_layer]))\n",
    "        \n",
    "        try:\n",
    "            # server can deserialize the bytes\n",
    "            ckks_vector = ts.ckks_vector_from(server_ts_ctx, model[encrypted_layer])\n",
    "\n",
    "            # this is supposed to fail with the available server context as it doesn't hold a secret key!\n",
    "            ckks_vector.decrypt()\n",
    "        except Exception as e:\n",
    "            print(f\"Server decryption failed with: {e}!\")\n",
    "            pass\n",
    "        \n",
    "        # However, the client can decrypt using its own TenSEAL context\n",
    "        ckks_vector = ts.ckks_vector_from(client_ts_ctx, model[encrypted_layer])\n",
    "        decrypted_params = ckks_vector.decrypt()\n",
    "        \n",
    "        print(f\"Client decrypted parameters for {encrypted_layer}:\")\n",
    "        np.set_printoptions(threshold=10) # don't show all values\n",
    "        print(np.asarray(decrypted_params))\n",
    "        \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Done \n",
    "\n",
    "Congratulations! You have trained and evaluated an FL model using secure aggregation with homomrophic encryption."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": "<!--- SPDX-License-Identifier: Apache-2.0 -->\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
