{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "\n# Bring your own Trainer to Clara FL \n\nIn V4 Clara FL allows you to bring your own trainer that could be anything.\nTherefore it does not need to use clara train. \nThis Notebook shows how to use clara FL using a MONAL trainer.\nFollowing this steps you could then change it to bring your own Tensorflow trainer \nor any Non Deep learning algorithm \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Resources\nYou could watch the free GTC 2021 talks covering Clara Train SDK\n- [Clara Train 4.0 - 101 Getting Started [SE2688]](https://gtc21.event.nvidia.com/media/Clara%20Train%204.0%20-%20101%20Getting%20Started%20%5BSE2688%5D/1_0qgfrql2)\n- [Clara Train 4.0 - 201 Federated Learning [SE3208]](https://gtc21.event.nvidia.com/media/Clara%20Train%204.0%20-%20201%20Federated%20Learning%20%5BSE3208%5D/1_m48t6b3y)\n- [Whatâ€™s New in Clara Train 4.0 [D3114]](https://gtc21.event.nvidia.com/media/What%E2%80%99s%20New%20in%20Clara%20Train%204.0%20%5BD3114%5D/1_umvjidt2)\n- [Take Medical AI from Concept to Production using Clara Imaging [S32482]](https://gtc21.event.nvidia.com/media/Take%20Medical%20AI%20from%20Concept%20to%20Production%20using%20Clara%20Imaging%20%20%5BS32482%5D/1_6bvnvyg7)\n- [Federated Learning for Medical AI [S32530]](https://gtc21.event.nvidia.com/media/Federated%20Learning%20for%20Medical%20AI%20%5BS32530%5D/1_z26u15uk)\n- [Get Started Now on Medical Imaging AI with Clara Train on Google Cloud Platform [S32518]](https://gtc21.event.nvidia.com/media/Get%20Started%20Now%20on%20Medical%20Imaging%20AI%20with%20Clara%20Train%20on%20Google%20Cloud%20Platform%20%5BS32518%5D/1_2yjdekmi)\n- [Automate 3D Medical Imaging Segmentation with AutoML and Neural Architecture Search [S32083]](https://gtc21.event.nvidia.com/media/Automate%203D%20Medical%20Imaging%20Segmentation%20with%20AutoML%20and%20Neural%20Architecture%20Search%20%5BS32083%5D/1_r5swh2jn)\n- [A Platform for Rapid Development and Clinical Translation of ML Models for Applications in Radiology at UCSF [S31619]](https://gtc21.event.nvidia.com/media/A%20Platform%20for%20Rapid%20Development%20and%20Clinical%20Translation%20of%20ML%20Models%20for%20Applications%20in%20Radiology%20at%20UCSF%20%5BS31619%5D/1_oz8qop5a)\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Brief Introduction\n\nWe are providing scripts to get started with BYO Trainer in a similar experience as we showed using clara train sdk.\nIf you are using clara train sdk docker image, you will need to shut it down.\nYou would follow steps below that will launch a new jupyter lab based on MONAI docker and not clara train \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Build Docker Image \n\nWe need to build a docker image for the experiment. \nThis docker should contain the framework that would run on the client and server. \nYou simply need to run `startDocker.sh` this will:\n- build a docker image based on MONAI\u0027s latest docker image in dockerhub\n- Install the fl wheel file \n- Install zip utility\n- tag the image as `projectmonai/monaiwfl:0.1`\n\nSteps above are controlled from the [Dockerfile](FLBYOTrainer/docker/Dockerfile)\n \nIn the end the `startDocker.sh` will launch a jupyter lab you should see output as \n```\n[I 16:49:04.476 LabApp] Serving notebooks from local directory: /claraDevDay\n[I 16:49:04.476 LabApp] Jupyter Notebook 6.2.0 is running at:\n[I 16:49:04.476 LabApp] http://hostname:8888/?token\u003d43f3dbe2acfab1f4412279d7a7b40d8471e5c966845299ea\n[I 16:49:04.476 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 16:49:04.480 LabApp] \n    To access the notebook, open this file in a browser:\n        file:///root/.local/share/jupyter/runtime/nbserver-1-open.html\n    Or copy and paste this URL:\n        http://hostname:8888/?token\u003d43f3dbe2acfab1f4412279d7a7b40d8471e5c966845299ea\n```\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Provision Package Preparation\n\nYou can now use the `provision` command to provision your FL experiment.\nYou can still use the same [Provisioning Notebook](../Provisioning.ipynb) to generate the packages\nPlease note that you are in a different container than clara train so you would need to redo the provisioning step.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Run Admin client \n\nOne you extracted the packages, you can follow along the [Admin Notebook](Admin.ipynb).\nOnly difference is that you upload the [adminMMAR_BYOT_monai Folder](adminMMAR_BYOT_monai) instead of [adminMMAR Folder](adminMMAR)\n\nIn case you would like to write your own trainer \nto work with Tensor Flow or non deep learning algorithms,\nYou should checkout the Trainer class\n\n```\nclass MONAITrainer(Trainer):\n    \"\"\"\n    This class implements a MONAI based trainer that can be used for Federated Learning.\n    Args:\n        aggregation_epochs: the number of training epochs for a round.\n            This parameter only works when `aggregation_iters` is 0. Defaults to 1.\n        aggregation_iters:  the number of training iterations for a round.\n            If the value is larger than 0, the trainer will use iteration based aggregation\n            rather than epoch based aggregation. Defaults to 0.\n    \"\"\"\n\n    def __init__(self, aggregation_epochs: int \u003d 1, aggregation_iters: int \u003d 0):\n\n    def _initialize_trainer(self, fl_ctx: FLContext):\n        \"\"\"\n        The trainer\u0027s initialization function. At the beginning of a FL experiment,\n        the train and evaluate engines, as well as train context and FL context\n        should be initialized.\n        \"\"\"\n\n    def handle_event(self, event_type: str, fl_ctx: FLContext):\n        \"\"\"\n        This function is an extended function from the super class.\n        It is used to perform the handler process based on the\n        event_type. At the start point of a FL experiment, necessary\n        components should be initialized. At the end of the experiment,\n        the running engines should be terminated.\n\n        Args:\n            event_type: the type of event that will be fired. In MONAITrainer,\n                only `START_RUN` and `END_RUN` need to be handled.\n            fl_ctx: an `FLContext` object.\n        \"\"\"\n    def train(self, shareable: Shareable, fl_ctx: FLContext, abort_signal: Signal) -\u003e Shareable:\n        \"\"\"\n        This function is an extended function from the super class.\n        As a supervised learning based trainer, the train function will run\n        evaluate and train engines based on model weights from `shareable`.\n        After fininshing training, a new `Shareable` object will be submitted\n        to server for aggregation.\n\n        Args:\n            shareable: the `Shareable` object acheived from server.\n            fl_ctx: the `FLContext` object achieved from server.\n            abort_signal: if triggered, the training will be aborted.\n\n        Returns:\n            a new `Shareable` object to be submitted to server for aggregation.\n        \"\"\"\n```\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "\u003c!--- SPDX-License-Identifier: Apache-2.0 --\u003e\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}