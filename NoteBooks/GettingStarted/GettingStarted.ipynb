{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Creating New MMARs for your New Project with Clara Train SDK\nClara Train SDK consists of different modules as shown below \n\u003cbr\u003e![side_bar](screenShots/TrainBlock.png)\n\nBy the end of this notebook you will:\n1. Understand components of [Medical Model ARchive (MMAR)](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/mmar.html)\n2. Know how to configure train config json to train a CNN\n3. Train a CNN with single and muiltple GPUS\n4. Fine tune a model\n5. Export a model \n6. Perform inference on testing dataset \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Prerequisites\n- Nvidia GPU with 8Gb of memory   ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Resources\nYou could watch the free GTC 2020 talks covering Clara Train SDK \n- [S22563](https://developer.nvidia.com/gtc/2020/video/S22563)\nClara train Getting started: cover basics, BYOC, AIAA, AutoML \n  ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Lets get started\nBefore we get started lets check that we have an NVIDIA GPU available in the docker by running the cell below",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# following command should show all gpus available \n!nvidia-smi",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Next cell defines functions that we will use throughout the notebook",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "MMAR_ROOT\u003d\"/claraDevDay/GettingStarted/\"\nprint (\"setting MMAR_ROOT\u003d\",MMAR_ROOT)\n%ls $MMAR_ROOT\n\n!chmod 777 $MMAR_ROOT/commands/*\ndef printFile(filePath,lnSt,lnOffset):\n    print (\"showing \",str(lnOffset),\" lines from file \",filePath, \"starting at line\",str(lnSt))\n    lnOffset\u003dlnSt+lnOffset\n    !\u003c $filePath head -n \"$lnOffset\" | tail -n +\"$lnSt\"",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Medical Model ARchive (MMAR)\nClara Train SDK uses the [Medical Model ARchive (MMAR)](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/mmar.html). \nThe MMAR defines a standard structure for organizing all artifacts produced during the model development life cycle. \nClara Train SDK simple basic idea is to train using config file as shown below\n\u003cbr\u003e![side_bar](screenShots/MMAR.png)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "You can download sample models for different problems from [NGC](https://ngc.nvidia.com/catalog/models?orderBy\u003dmodifiedDESC\u0026pageNumber\u003d0\u0026query\u003dclara\u0026quickFilter\u003d\u0026filters\u003d) \u003cbr\u003e \nAll MMAR follow the structure provided in this Notebook. if you navigate to the parent folder structure it should contain the following subdirectories\n```\n./GettingStarted \n├── commands\n├── config\n├── docs\n├── eval\n├── models\n└── resources\n```\n\n* `commands` contains a number of ready-to-run scripts for:\n    - training\n    - training with multiple GPUS\n    - validation\n    - inference (testing)\n    - exporting models in TensorRT Inference Server format\n* `config` contains configuration files (in JSON format) for eac training, \nvalidation, and deployment for [AI-assisted annotation](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/aiaa/index.html) \n(_Note:_ these configuration files are used in the scripts under the `commands` folder)\n* `docs` contains local documentation for the model, but for a more complete view it is recommended you visit the NGC model page\n* `eval` is used as the output directory for model evaluation (by default)\n* `models` is where the tensorflow checkpoint-formatted model is stored (`.index`, `.meta`, `.data-xxxxx-of-xxxxx`), and the corresponding graph definition files (`fzn.pb` for frozen models, and `trt.pb` for TRT models)\n* `resources` currently contains the logger configuration in `log.config` file\n\nSome of the most important files you will need to understand to configure and use Clara Train SDK are\n\n1. `environment.json` which has important common parameters to set the path for \n    * `DATA_ROOT` is the root folder where the data with which we would like to train, validate, or test resides in\n    * `DATASET_JSON` expects the path to a JSON-formatted file \n    * `MMAR_CKPT_DIR` the path to the where the tensorflow checkpoint files reside\n    * `MMAR_EVAL_OUTPUT_PATH` the path to output evaluation metrics for the neural network during training, validation, and inference\n    * `PROCESSING_TASK` the type of processing task the neural net is intended to perform (currently limited to `annotation`, `segmentation`, `classification`)\n    * `PRETRAIN_WEIGHTS_FILE` (_optional_) \tdetermines the location of the pre-trained weights file; if the file does not exist and is needed, \n    the training program will download it from predefined URL from the web\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "printFile(MMAR_ROOT+\"/config/environment.json\",0,30)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "2. `train.sh` and `train_finetune.sh` run the command to train the network based on the `config_train.json` configuration; \nthis shell script can be also used to override parameters in `config_train.json` using the `--set` argument (see `train_finetune.sh`)\n\n_Note_: The main difference between the two is that `train_finetune.sh` specifies a `ckpt` file, \nwhile `train.sh` does not since it is training from scratch.\n\nLet\u0027s take a look at `train.sh` by executing the following cell.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "printFile(MMAR_ROOT+\"/commands/train_W_Config.sh\",30,30)\nprintFile(MMAR_ROOT+\"/commands/train_finetune.sh\",0,30)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Config.json Main Concepts ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n`config_train.json` contains all the parameters necessary to define the neural net, \nhow is it trained (training hyper-parameters, loss, etc.), \npre- and post-transformation functions necessary to modify and/or augment the data before input to the neural net, etc. \nThe complete documentation on the training configuration is laid out \n[here](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/appendix/configuration.html#training-configuration).\nConfiguration file defines all training related configurations. \nThis is were most the researcher would spent most of his time.\n\n\u003cbr\u003e![s](screenShots/MMARParts.png)\u003cbr\u003e \n\nLets take some time to examine each part of it.  \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "1. Global configurations ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \"model\": {\n",
            "      \"name\": \"Unet\",\n",
            "      \"args\": {\n",
            "        \"num_classes\": 6,\n",
            "        \"nf_enc\":\"32,64,64,64\",\n",
            "        \"nf_dec\":\"64,64,64,64,64,32,32\"\n",
            "      }\n",
            "    },\n",
            "    \"pre_transforms\": [\n",
            "      {\n",
            "        \"name\": \"LoadNifti\",\n"
          ]
        }
      ],
      "source": "confFile\u003dMMAR_ROOT+\"/config/trn_base.json\"\nprintFile(confFile,0,10)"
    },
    {
      "cell_type": "markdown",
      "source": "\n2. Training config which includes:\n    1. Loss functions:\n    [Dice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003ddice#module-ai4med.components.losses.dice)\n    , [CrossEntropy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003dcrossentropy#ai4med.components.losses.cross_entropy.CrossEntropy)\n    , [Focal](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003dfocal#module-ai4med.components.losses.focal)\n    , [FocalDice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003dfocaldice#ai4med.components.losses.focal_dice.FocalDice) \n    , [CrossEntropyDice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003dcrossentropydice#ai4med.components.losses.cross_entropy_dice.CrossEntropyDice) \n    , [BinaryClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003dbinaryclassificationloss#ai4med.components.losses.classification_loss.BinaryClassificationLoss)\n    , [MulticlassClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003dmulticlassclassificationloss#ai4med.components.losses.classification_loss.MulticlassClassificationLoss)\n    , [WeightedMulticlassClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight\u003dweightedmulticlassclassificationloss#ai4med.components.losses.classification_loss.WeightedMulticlassClassificationLoss)\n    2. Optimizer\n    [Momentum]()\n    , [Adam]()\n    , [NovaGrad]()\n    3. Network architecture\n    [SegAhnet]()\n    , [SegResnet]()\n    , [Unet]()\n    , [UnetParallel]()\n    , [DenseNet121]()\n    , [Alexnet]()\n    4. Learing rate Policy \n    [ReducePoly]()\n    , [DecayOnStep]()\n    , [ReduceCosine]()\n    , [ReduceOnPlateau]()\n    5. Image pipeline\n        1. Classification \n        , [ClassificationImagePipeline]()\n        , [ClassificationImagePipelineWithCache]()\n        , [ClassificationKerasImagePipeline]()\n        , [ClassificationKerasImagePipelineWithCache]()\n        2. Segmenatation \n        , [SegmentationImagePipeline]()\n        , [SegmentationImagePipelineWithCache]()\n        , [SegmentationKerasImagePipeline]()\n        , [SegmentationKerasImagePipelineWithCache]()    \n    4. Pretransforms\n        1. Loading transformations:\n            [LoadNifti](https://docs.nvidia.com/clara/tlt-m[i/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dloadnifti#ai4med.components.transforms.load_nifti.LoadNifti)\n            , [LoadPng](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dloadpng#ai4med.components.transforms.load_png.LoadPng)\n            , [ConvertToChannelsFirst](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dconverttochannelsfirst#ai4med.components.transforms.convert_to_channels_first.ConvertToChannelsFirst)\n            , [LoadImageMasksFromNumpy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dloadimagemasksfromnumpy#ai4med.components.transforms.load_image_masks_from_numpy.LoadImageMasksFromNumpy)\n            , [LoadJpg](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dloadjpg#ai4med.components.transforms.load_jpg.LoadJpg)\n        2. Resample Transformation\n            [RepeatChannel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003drepeatchannel#ai4med.components.transforms.repeat_channel.RepeatChannel)\n            , [ScaleByFactor](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dscalebyfactor#ai4med.components.transforms.scale_by_factor.ScaleByFactor)\n            , [ScaleByResolution](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dscalebyresolution#ai4med.components.transforms.scale_by_resolution.ScaleByResolution)\n            , [ScaleBySpacing](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dscalebyspacing#ai4med.components.transforms.scale_by_spacing.ScaleBySpacing)\n            , [ScaleToShape](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dscaletoshape#ai4med.components.transforms.scale_to_shape.ScaleToShape)\n            , [RestoreOriginalShape](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003drestoreoriginalshape#ai4med.components.transforms.restore_original_shape.RestoreOriginalShape)\n            , [LoadImageMasksFromNumpy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dloadimagemasksfromnumpy#ai4med.components.transforms.load_image_masks_from_numpy.LoadImageMasksFromNumpy)\n        3. Cropping transformations\n            [CropForegroundObject](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcropforegroundobject#ai4med.components.transforms.crop_foreground_object.CropForegroundObject)\n            , [FastPosNegRatioCropROI](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dfastposnegratiocroproi#ai4med.components.transforms.fast_pos_neg_ratio_crop_roi.FastPosNegRatioCropROI)\n            , [CropByPosNegRatio](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcropbyposnegratio#ai4med.components.transforms.crop_by_pos_neg_ratio.CropByPosNegRatio)\n            , [SymmetricPadderDiv](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dsymmetricpadderdiv#ai4med.components.transforms.symmetric_padder_div.SymmetricPadderDiv)\n            , [FastCropByPosNegRatio](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dfastcropbyposnegratio#ai4med.components.transforms.fast_crop_by_pos_neg_ratio.FastCropByPosNegRatio)\n            , [CropByPosNegRatioLabelOnly](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcropbyposnegratiolabelonly#ai4med.components.transforms.crop_by_pos_neg_ratio_label_only.CropByPosNegRatioLabelOnly)\n            , [CropForegroundObject](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcropforegroundobject#ai4med.components.transforms.crop_foreground_object.CropForegroundObject)\n            , [CropSubVolumeCenter](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcropsubvolumecenter#ai4med.components.transforms.crop_sub_volume_center.CropSubVolumeCenter)\n            , [CropRandomSizeWithDisplacement](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcroprandomsizewithdisplacement#ai4med.components.transforms.crop_random_size_w_displacement.CropRandomSizeWithDisplacement)\n            , [CropFixedSizeRandomCenter](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcropfixedsizerandomcenter#ai4med.components.transforms.crop_fixed_size_random_center.CropFixedSizeRandomCenter)\n        4. Deformable transformations\n            [FastPosNegRatioCropROI](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dfastposnegratiocroproi#ai4med.components.transforms.fast_pos_neg_ratio_crop_roi.FastPosNegRatioCropROI)\n        5. Intensity Transforms\n            [ScaleIntensityRange](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dscaleintensityrange#ai4med.components.transforms.scale_intensity_range.ScaleIntensityRange)\n            , [ScaleIntensityOscillation](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dscaleintensityoscillation#ai4med.components.transforms.scale_intensity_oscillation.ScaleIntensityOscillation)\n            , [AddGaussianNoise](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003daddgaussiannoise#ai4med.components.transforms.add_gaussian_noise.AddGaussianNoise)\n            , [NormalizeNonzeroIntensities](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dnormalizenonzerointensities#ai4med.components.transforms.normalize_nonzero_intensities.NormalizeNonzeroIntensities)\n            , [CenterData](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcenterdata#ai4med.components.transforms.center_data.CenterData)\n            , [AdjustContrast](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dadjustcontrast#ai4med.components.transforms.adjust_contrast.AdjustContrast)\n            , [RandomGaussianSmooth]()\n            , [RandomMRBiasField]()\n        6. Augmentation Transforms\n            [RandomZoom](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003drandomzoom#ai4med.components.transforms.random_zoom.RandomZoom)\n            , [RandomAxisFlip](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003drandomaxisflip#ai4med.components.transforms.random_axis_flip.RandomAxisFlip)\n            , [RandomSpatialFlip](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003drandomspatialflip#ai4med.components.transforms.random_spatial_flip.RandomSpatialFlip)\n            , [RandomRotate2D](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003drandomrotate2d#ai4med.components.transforms.random_rotate_2d.RandomRotate2D)\n            , [RandomRotate3D](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003drandomrotate3d#ai4med.components.transforms.random_rotate_3d.RandomRotate3D)\n        7. Special transforms \n            [AddExtremePointsChannel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003daddextremepointschannel#ai4med.components.transforms.add_extreme_points_channel.AddExtremePointsChannel)\n            , [SplitAcrossChannels](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dsplitacrosschannels#ai4med.components.transforms.split_across_channels.SplitAcrossChannels)\n            , [SplitBasedOnLabel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dsplitbasedonlabel#ai4med.components.transforms.split_based_on_label.SplitBasedOnLabel)\n            , [ThresholdValues](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dthresholdvalues#ai4med.components.transforms.apply_threshold.ThresholdValues)\n            , [SplitBasedOnBratsClasses](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dsplitbasedonbratsclasses#ai4med.components.transforms.split_based_on_brats_classes.SplitBasedOnBratsClasses)\n            , [ConvertToMultiChannelBasedOnLabel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dconverttomultichannelbasedonlabel#ai4med.components.transforms.convert_to_multi_channel_based_on_label.ConvertToMultiChannelBasedOnLabel)\n            , [KeepLargestCC](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dkeeplargestcc#ai4med.components.transforms.keep_largest_connected_component.KeepLargestCC)\n            , [CopyProperties](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dcopyproperties#ai4med.components.transforms.copy_properties.CopyProperties)\n            , [ConvertToMultiChannelBasedOnBratsClasses](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dconverttomultichannelbasedonbratsclasses#ai4med.components.transforms.convert_to_multi_channel_based_on_brats_classes.ConvertToMultiChannelBasedOnBratsClasses)\n            , [ArgmaxAcrossChannels](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight\u003dargmaxacrosschannels#ai4med.components.transforms.argmax_across_channels.ArgmaxAcrossChannels)       \n\n        ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "confFile\u003dMMAR_ROOT+\"/config/trn_base.json\"\nprintFile(confFile,9,8)\nprintFile(confFile,16,8)\nprintFile(confFile,25,20)\nprintFile(confFile,108,15)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n\n3. Validation config which includes:\n    1. Metric \n    2. pre-transforms. Since these transforms are usually a subset from teh pre-transforms in the training section, \n    we can use the alias to point to these transforms by name as ` \"ref\": \"LoadNifti\"`. \n    In case we use 2 transforms with the same name as `ScaleByResolution` \n    we can give each an alias to refer to as `\"name\": \"ScaleByResolution#ScaleImg\"` \n    then refer to it in the validation section as `ScaleImg` \n    3. Image pipeline\n    4. Inference",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "confFile\u003dMMAR_ROOT+\"/config/trn_base.json\"\nprintFile(confFile,120,13)\nprintFile(confFile,135,16)\nprintFile(confFile,152,12)\nprintFile(confFile,164,10)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## Start TensorBoard \nBefore we start training or while the network is training, \nyou can monitor its accuracy using tensorboard in side jupyter lab as shown below\n \u003cbr\u003e![tb](screenShots/TensorBoard.png)\u003cbr\u003e \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Lets start training\nNow that we have our training configuration, to start training simply run `train.sh` as below. \nPlease keep in mind that we have setup a dummy data with one file to train a dummy network fast (we only train for 2 epochs). \nPlease see exercises on how to easily switch data and train a real segmentation network.\n\n**_Note:_** We have renamed `train.sh` to `train_W_Config` as we modified it to accept parameters with the config to use       ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! $MMAR_ROOT/commands/train_W_Config.sh trn_base.json",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Now lets see the `models` directory, which would includes out models and the tensorboard files ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! ls -la $MMAR_ROOT/models/trn_base",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n\n## Export Model\n\nTo export the model we simply run `export.sh` which will: \n- Removes back propagation information from checkpoint files\n- Generates two frozen graphs in the models folder\nThis optimized model will be used by TRTIS server in Clara Deploy.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! $MMAR_ROOT/commands/export.sh",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n\nlets check out what was created in the folder. \nafter running cell below you should see:\n1. Frozen File Generated: /claraDevDay/GettingStarted/commands/../models/trn_base/model.fzn.pb\n2. TRT File Generated: /claraDevDay/GettingStarted/commands/../models/trn_base/model.trt.pb\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "!ls -la $MMAR_ROOT/models/trn_base/*.pb",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n## Evaluate and Prediction \nNow that we have trained our model we would like to run evaluation to get some statistics and also do inference to see the resulted output\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1. Evaluate \nTo run evaluation on your validation dataset you should run `validate.sh`. \nThis will run evaluation on the validation dataset and place it in the `MMAR_EVAL_OUTPUT_PATH` as configured in the [environment.json](config/environment.json) \nfile (default is eval folder). \nThis evaluation would give min, max, mean of the metric as specified in the config_validation file\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! $MMAR_ROOT/commands/validate.sh",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Now lets see results in the folder by running cells below. \nYou should see statistics and dice per file in the validation dataset",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! ls -la $MMAR_ROOT/eval/",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# statistic summary\n!cat $MMAR_ROOT/eval/mean_dice_class1_summary_results.txt",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "!cat $MMAR_ROOT/eval/mean_dice_class1_raw_results.txt",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2. Predict\n\nTo run inference on validation dataset or test dataset you should run `infer.sh`. \nThis will run prediction on the validation dataset and place it in the `MMAR_EVAL_OUTPUT_PATH` as configured in the \n[environment.json](config/environment.json) file (default is eval folder)\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! $MMAR_ROOT/commands/infer.sh",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Now lets see results in the folder",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! ls -la $MMAR_ROOT/eval/",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! ls -la $MMAR_ROOT/eval/spleen_8",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Multi-GPU Training\nClara train aims to simplify scaling and utilizing all available gpus. \nUsing the same config we already used for train we can simply invoke `train_2gpu.sh` to train on multiple gpus. \nWe are using MPI and Horovod to speed up training and passing weights between GPUs as shown below\n\u003cbr\u003e![tb](screenShots/MultiGPU.png)\u003cbr\u003e\n\nLets examine `train_2gpu.sh` script by running cell below. \nYou can see we are changing the learning rate as the batch size has doubled.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "printFile(MMAR_ROOT+\"/commands/train_2gpu.sh\",0,50)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Lets give it a try and run cell below to train on 2 gpus",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "! $MMAR_ROOT/commands/train_2gpu.sh",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n\n# Exercise:\nNow that you are familiar with clara train, you can try to: \n1. Explore different options of clara train by changing / creating a new config file and running training: \n    1. Model architecture: Ahnet, Unet, Segresnet \n    2. Losses\n    3. Transformation \n2. Train on real spleen data for this you should:\n    1. Download spleen dataset by running the [download](download) Notebook\n    2. Switch the dataset file in the [environment.json](config/environment.json)\n    3. rerun the `train.sh`\n3. Experiment with multi-GPU training by changing number of gpus to train on from 2 to 3 or 4. \nYou should edit [train_2gpu.sh](commands/train_2gpu.sh) then rerun the script \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "4. Use DLprof tool for debugging ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "!$MMAR_ROOT/commands/debug_dlprof.sh trn_base.json",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "You then need to run tensor board manually (Not through jupyterlab) using \n```\ncd /claraDevDay/GettingStarted/models/trn_base_debug\ntensorboard --logdir ./dlprof --port 80\n```\nremember we startdocker.sh mounted port 80 to 5000 by default for AIAA, we simply are using that mapping here for simplicity \nnow if you navigate to `\u003cyourip:5000\u003e` you should see DlProf tool as below. \nThis analysis shows you the GPUs you have along improvements that you can do to train faster. \nFor example this run shows multiple operations that would be accelerated from AMP.\nTo test this you can run cell below with AMP enabled in the configuration \n\n\u003cbr\u003e![dlprof](screenShots/Dlprof.png)\u003cbr\u003e\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "!$MMAR_ROOT/commands/debug_dlprof.sh trn_base_AMP.json",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Advanced Exercise for K8s:\nIn case your system has K8s cluster and you would like to run clara train jobs, you should try the following:\n1. Run a K8s job using 1 gpu by running `kubectl apply -f ./config/k8config.yaml`\n2. Run a K8s job using 2 (or N gpus) by running `kubectl apply -f ./config/k8config2gpu.yaml`\n\nyou may need to delete the pod with the `clarak8` name to re-run using `kubectl delete pod clarak8`.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Advanced Exercise Connect train to deploy\nYou can export model to be ready to be consumed by clara deploy. \nYou can use [export2Triton.sh](commands/export2Triton.sh) script to generate the model as triton expects it (used by clara deploy)\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "!$MMAR_ROOT/commands/export2Triton.sh ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}